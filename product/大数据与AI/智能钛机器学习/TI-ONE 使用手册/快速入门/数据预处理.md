除了在程序中对数据做预处理，在上游系统对数据作SQL操作，平台的PySpark组件也可以实现简单的SQL处理。

Spark 组件是面向使用Python的Spark用户，用户通过Python编写Spark应用程序，通过PySpark组件完成部署，也支持pyspark的sql功能，本文有部分使用方法介绍（更多用法请参考社区指引：[Spark SQL, DataFrames and Datasets Guide](http://spark.apache.org/docs/latest/sql-programming-guide.html)）。

和标准的Spark相比，pySpark 支持上传 Python 脚本和实时修改，更加的灵活，而且支持SQL功能，所以我们推荐用来进行数据预处理。

从左侧组件列表里拖拽出一个 PySpark 节点：


![](https://main.qcloudimg.com/raw/623bc1e003d404c5efea462a67af0d30.png)


单击任务节点，会从右侧弹出配置框：


![](https://main.qcloudimg.com/raw/98c028f8b29cf69b9ae102e0116bf988.png)

### 入门示例

首先通过一个简单的例子来看看如何使用 Python 写 Spark 应用程序，如下代码显示通过`range(1, 4)`初始化构造弹性分布式数据集`RDD`，经过 transformation 方法`map`转换后得到一种新的形式，最后调用 action 方法`collect`收集结果并打印。

 ![](https://main.qcloudimg.com/raw/999e90aa6c9022609527d6eebbf1a54f.png)

### RDD Style

下面代码显示基于 RDD API，从 COS 上读取原始数据，经过一系列转换处理后，得到每条记录的前 4 列，然后通过 action 方法`take(2)`仅拿出两条记录打印出来看看处理效果，如果处理正确，可以通过`saveAsTextFile`将结果保存到 COS 结果目录中。

![](https://main.qcloudimg.com/raw/37fb38a2a08b8c048045c8c04e963126.png)

### DataFrame Style

上述基于 RDD 的代码可能对数据分析人员来说并不是很好懂，用过 scikit-learn 的同学可能会对 dataframe 更熟悉，下面代码显示直接通过 spark 的提供的 csv 接口加载源数据得到 dataframe，并打印schema信息，选出”age”和”marital”两列，显示 4 条记录。

![](https://main.qcloudimg.com/raw/e7b70669e366eb32cbbde584c0b7f21f.png)

从以上代码可以看到，通过 csv 直接加载得到的dataframe，”age”列是`string`类型，我们可以使用`cast`表达式将其转换为`int`类型。
![](https://main.qcloudimg.com/raw/b021cea7a97edda53ca5ba1a425bfd64.png)

基于 dataframe，可以使用 SQL 算子灵活方便地处理数据，下面代码显示使用`groupBy`对数据进行分组计算。
![](https://main.qcloudimg.com/raw/beae07af678fc57c13daf45a90594a65.png) 

当然，你还可以像如下代码显示一样直接写 SQL。 
![](https://main.qcloudimg.com/raw/daf9b5e0a8731f01dc93eb4921c0e7ed.png)
